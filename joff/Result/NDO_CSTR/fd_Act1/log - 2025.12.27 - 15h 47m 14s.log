
NDO(
  (loss_func): MSELoss()
  (seq): Sequential(
    (layer_0): Layer(
      (0): Linear(in_features=52, out_features=200, bias=True)
      (1): Gaussian()
    )
    (layer_1): Layer(
      (0): Dropout(p=0.0133, inplace=False)
      (1): Linear(in_features=200, out_features=150, bias=True)
      (2): Sigmoid()
    )
    (layer_2): Layer(
      (0): Dropout(p=0.01, inplace=False)
      (1): Linear(in_features=150, out_features=50, bias=True)
      (2): Gaussian()
    )
  )
)

run paras: {'run_times': 1, 'multirun_seed': 'r*2', '_run_id': '', 'e': 1, 'load_para_type': None, 'load_sub': None, 'load_file_name': None, 'load_path_replace': None, 'view_addi_info': ['observer_loss', 'stability_loss', 'loss'], 'if_log': True, 'if_epoch_eval': None, 'if_vmap': False, 'if_save_excel': True, 'if_save_plot': True, 'fd_pr': ['cust_mm_ey-T2&Q-kde'], 'fd_pr_dt': [{'mm': 'cust_mm_ey', 'ts': 'T2', 'thrd_meth': 'kde'}, {'mm': 'cust_mm_ey', 'ts': 'Q', 'thrd_meth': 'kde'}], 'fd_pr_name': ['cust_mm_ey-T2-kde', 'cust_mm_ey-Q-kde'], 'expt_FAR': 0.005, 'ts_post_op': None, 'if_use_lstsq': True, 'if_plot_mm_kde': False, 'if_plot_score_hm': False, 'if_plot_cov_inv': True, 'if_save_ts': False, 'alld_error': 0.005, 'cl': 0.995, 'if_intp': False}

Train NDO_CSTR/fd_Act1 via cuda:
Traceback (most recent call last):
  File "F:\torch_joff\joff\_run\_run_model.py", line 67, in _run_with_log
    _run_n(self, p)
  File "F:\torch_joff\joff\_run\_run_model.py", line 111, in _run_n
    train_model(self, **p)
  File "F:\torch_joff\joff\_run\_run_model.py", line 199, in train_model
    else: batch_training(self)
          ^^^^^^^^^^^^^^^^^^^^
  File "F:\torch_joff\joff\_run\_epoch.py", line 30, in batch_training
    self.forward(input)
  File "F:\torch_joff\joff\_model\arx.py", line 215, in forward
    J = self.get_vmap_jac(x_pre, u, h_pres, c_pres)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "F:\torch_joff\joff\_model\arx.py", line 134, in get_vmap_jac
    J = vmap_jac(dynamics_fn, 0, self.in_dims, *args).detach()
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "F:\torch_joff\joff\_model\arx.py", line 37, in vmap_jac
    J = batch_jac_fn(*args)
        ^^^^^^^^^^^^^^^^^^^
  File "F:\anaconda3\Lib\site-packages\torch\_functorch\apis.py", line 203, in wrapped
    return vmap_impl(
           ^^^^^^^^^^
  File "F:\anaconda3\Lib\site-packages\torch\_functorch\vmap.py", line 331, in vmap_impl
    return _flat_vmap(
           ^^^^^^^^^^^
  File "F:\anaconda3\Lib\site-packages\torch\_functorch\vmap.py", line 479, in _flat_vmap
    batched_outputs = func(*batched_inputs, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "F:\anaconda3\Lib\site-packages\torch\_functorch\eager_transforms.py", line 604, in wrapper_fn
    vjp_out = _vjp_with_argnums(func, *args, argnums=argnums, has_aux=has_aux)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "F:\anaconda3\Lib\site-packages\torch\_functorch\vmap.py", line 48, in fn
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "F:\anaconda3\Lib\site-packages\torch\_functorch\eager_transforms.py", line 399, in _vjp_with_argnums
    primals_out = func(*primals)
                  ^^^^^^^^^^^^^^
  File "F:\torch_joff\joff\_model\arx.py", line 132, in <lambda>
    dynamics_fn = lambda *args: self.dynamics_fn(*args)[0]
                                ^^^^^^^^^^^^^^^^^^^^^^^
  File "F:\torch_joff\joff\_model\arx.py", line 177, in dynamics_fn
    a = layer(a, exd_h, c)
        ^^^^^^^^^^^^^^^^^^
  File "F:\anaconda3\Lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "F:\anaconda3\Lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "F:\torch_joff\joff\_model\seq.py", line 30, in forward
    x = cell(x, h0, c0)
        ^^^^^^^^^^^^^^^
  File "F:\anaconda3\Lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "F:\anaconda3\Lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "F:\torch_joff\joff\_model\cell.py", line 14, in forward
    x = torch.cat([args[0], args[1]], dim=-1) \
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument tensors in method wrapper_CUDA_cat)
